{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 - Sampling & Reliability\n",
    "\n",
    "Up until this week, we have assumed that the corpus you have used for analysis assignments represented a *meaningful* assemblage of texts from which reasonable inferences could be drawn about the social game, social world and social actors that produced it. \n",
    "\n",
    "This week, we ask you to build a corpus for preliminary analysis and articulate what your sample represents in context of your final project. We begin by exploring how we can get *human* readings of content at scale. We want to gather and utilize human responses for several reasons. First, we may want to use crowdsourced human scores as the primary method of coding, extracting or organizing content (as it was in two of the assigned readings). Second, we may want to validate or tune a computational algorithm we may have developed in terms of how it is associated with human meanings or experience. Finally, we may want to use human coding on a sample of data as the basis for training a model or algorithm to then extrapolate *human-like* annotations to the entire population. Here intelligent sampling is critical to maximize effective maching training. \n",
    "\n",
    "For this notebook we will be using the following packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Special module written for this class\n",
    "#This provides access to data and to helper functions from previous weeks\n",
    "#Make sure you update it before starting this notebook\n",
    "import lucem_illud_2020 #pip install -U git+git://github.com/Computational-Content-Analysis-2020/lucem_illud_2020.git\n",
    "\n",
    "#All these packages need to be installed from pip\n",
    "import numpy as np #For arrays\n",
    "import scipy as sp #For some stats\n",
    "import pandas #Gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import seaborn #Makes the graphics look nicer\n",
    "import pyanno #On python3 make sure to pip install pyanno3\n",
    "\n",
    "#We need to import these this way due to how pyanno is setup\n",
    "from pyanno.measures import pairwise_matrix, agreement, cohens_kappa, cohens_weighted_kappa, fleiss_kappa, krippendorffs_alpha, pearsons_rho, scotts_pi, spearmans_rho\n",
    "from pyanno.annotations import AnnotationsContainer\n",
    "from pyanno.models import ModelA, ModelBt, ModelB\n",
    "\n",
    "from functools import reduce\n",
    "from itertools import permutations\n",
    "import math\n",
    "\n",
    "\n",
    "#This 'magic' command makes the plots work better\n",
    "#in the notebook, don't use it outside of a notebook.\n",
    "#Also you can ignore the warning\n",
    "%matplotlib inline\n",
    "\n",
    "import os #For looking through files\n",
    "import os.path #For managing file paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Pitch the Domain of your Project*</span>\n",
    "\n",
    "<span style=\"color:red\">In the two cells immediately following, describe **WHAT** you are planning to analyze for your final project (i.e., texts, contexts and the social game, world and actors you intend to learn about through your analysis) (<200 words) and **WHY** you are going to do it (i.e., why would theory and/or the average person benefit from knowing the results of your investigation) (<200 words)? [**Note**: your individual or collective project can change over the course of the quarter if new data and/or analysis opportunities arise or if old ones fade away.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***What?*** \n",
    "<200 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Why?***\n",
    "<200 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Pitch Your Sample*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cell immediately following, describe the rationale behind your proposed sample design for your final project. What is the social game, social work, or social actors about whom you are seeking to make inferences? What are its virtues with respect to your research questions? What are its limitations? What are alternatives? What would be a reasonable path to \"scale up\" your sample for further analysis (i.e., high-profile publication)? (<200 words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Annotation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Rzhetsky et al (2009)'s sample dataset, which can be found [here](https://github.com/enthought/uchicago-pyanno/tree/master/data). This data is the result of a content analytic / content extraction study in which Andrey Rzhetsky and colleagues from the National Library of Medicine, published [here](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000391) in [PLOS Computational Biology](http://journals.plos.org/ploscompbiol/), gave eight annotators 10,000 sentence chunks from biomedical text in biomedical abstracts and articles, then asked them, in a loop design schematically illustrated below that provided 3 independent codings for each document. The sampling strategy pursued diversity by drawing from PubMed abstracts (1000) and full-text articles (9000: 20% from abstracts, 10% from introductions, 20% from methods, 25% from results, and 25% from discussions.) The dataset extract here involves respondents codes for sentences in terms of their *Evidence*: {0, 1, 2, 3, -1} where 0 is the complete lack of evidence, 3 is direct evidence present within the sentence, and -1 is didn't respond. (They also crowdsourced and analyzed *polarity*, *certainty*, and *number*). For example, consider the following two abutting sentence chunks: *\"Because null mutations in toxR and toxT abolish CT and TcpA expression in the El Tor biotype and also attenuate virulence...\"* [i.e., average certainty = 0], *\"...it is likely that the ToxR regulon has functional similarities between the two biotypes despite the clear differences in the inducing parameters observed in vitro\"* [i.e., average certainty = 1].\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img source=\"loopdesign.png\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img source=\"loopdesign.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Click here for loop design](loopdesign.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.loadtxt(\"../data/pyAnno/testdata_numerical.txt\")\n",
    "anno = AnnotationsContainer.from_array(x, missing_values=[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interrogate the AnnotationsContainer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3, ..., -1, -1, -1],\n",
       "       [ 0,  0,  0, ..., -1, -1, -1],\n",
       "       [ 2,  2,  1, ..., -1, -1, -1],\n",
       "       ...,\n",
       "       [ 2,  2, -1, ..., -1, -1,  1],\n",
       "       [ 2,  2, -1, ..., -1, -1,  3],\n",
       "       [ 1,  1, -1, ..., -1, -1,  0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno.annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 2.0, 3.0, 4.0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno.missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we assume categorical codes...that each code is qualitatively distinct from each other. Two measures are primarily used for this: Scott's $\\pi$, Cohen's $\\kappa$, and Krippendorff's $\\alpha$ which each measure the extent of agreement between two annotators, but take into account the possibility of the agreement occurring by chance in slightly different ways. Any agreement measure begins with the frequency of codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11666667, 0.245     , 0.34083333, 0.2975    ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyanno.measures.agreement.labels_frequency(anno.annotations,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider the \"confusion matrix\" or matrix of coded agreements between any two coders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.  7.  3.  2.]\n",
      " [ 9. 30. 13.  3.]\n",
      " [ 2.  9. 42. 11.]\n",
      " [ 1.  7. 14. 39.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "c = pyanno.measures.agreement.confusion_matrix(anno.annotations[:,0], anno.annotations[:,1],4)\n",
    "print(c)\n",
    "ac = seaborn.heatmap(c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scott's $\\pi$ is computed as:\n",
    "\n",
    "$\\pi = \\frac{\\text{Pr}(a)-\\text{Pr}(e)}{1-\\text{Pr}(e)}$\n",
    "\n",
    "Where Pr($a$) is relative observed agreement, and Pr($e$) is expected agreement using joint proportions calculated from the confusion matrix or matrix of coded agreements between any two coders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scotts_pi(anno.annotations[:,0], anno.annotations[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generalization of Scott's $\\pi$ to $n$ coders is Fleiss' $\\kappa$ (Fleiss called it $\\kappa$ because he thought he was generalizing Cohen's $\\kappa$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fleiss_kappa(anno.annotations[::])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Krippendorff's $\\alpha$ generalizes of Fleiss' $\\kappa$ to $n$ coders and takes into account the fact that annotations here are not categorically different, but ordinal, by adding a weight matrix in which off-diagonal cells contain weights indicating the seriousness of the disagreement between each score. When produced with no arguments, it simply produces an arithmetic distance (e.g., 3-1=2), such that cells one off the diagonal are weighted 1, two off 2, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "krippendorffs_alpha(anno.annotations[::])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like Scott's $\\pi$, Cohen's $\\kappa$ also takes into account the possibility of the agreement occurring by chance, but in the following way:\n",
    "\n",
    "$\\kappa = \\frac{p_o-p_e}{1-p_e}=1-\\frac{1-p_o}{p_e}$\n",
    "\n",
    "where $p_o$ is the relative observed agreement among raters, and $p_e$ is the hypothetical probability of chance agreement, using the observed data to calculate the probabilities of each observer randomly saying each category. If the raters are in complete agreement then $\\kappa = 1$. If there is no agreement among the raters other than what would be expected by chance (as given by $p_e$), $\\kappa ≤ 0 $. Here, Cohen's $\\kappa$ statistic for the first two annotators is computed. This is probably the most common metric of agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohens_kappa(anno.annotations[:,0], anno.annotations[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = pairwise_matrix(cohens_kappa, anno.annotations)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = seaborn.heatmap(m)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that this 8 by 3 loop design will be less stable than an 8 choose 3 combinatorial design, because each codes with more others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can also assess the average Cohen's $\\kappa$ for all pairs of coders that have coded against one another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_metric_average(metric, array):\n",
    "    \"\"\"Calculate the pairwise metric average for the real elements of metric function run on an array of annotations\"\"\"\n",
    "    p = permutations(range(array[0,:].size),2)\n",
    "    m = [metric(array[:,x[0]], array[:,x[1]]) for x in p]\n",
    "    clean_m = [c for c in m if not math.isnan(c)]\n",
    "    return reduce(lambda a, b: a + b, clean_m)/len(clean_m)    \n",
    " \n",
    "pairwise_metric_average(cohens_kappa, anno.annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As recognized with Krippendorff's flexible $\\alpha$, our scores are *not* categorical, but rather ordered and her considered metric. Weighted $\\kappa$ allows you to count disagreements differently and is useful when codes are ordered as they are here. Here a weight matrix is added to the calculation, in which off-diagonal cells contain weights indicating the seriousness of the disagreement between each score. When automatically produced, it simply produces an arithmetic distance (e.g., 3-1=2), such that cells one off the diagonal are weighted 1, two off 2, etc. Here\n",
    "\n",
    "$\\kappa = 1-\\frac{\\sum^k_{i=1}\\sum^k_{j=1}w_{ij}x_{ij}}{\\sum^k_{i=1}\\sum^k_{j=1}w_{ij}m_{ij}}$\n",
    "\n",
    "where $\\kappa$ = $n$ codes and $w_{ij}$,$x_{ij}$, and $m_{ij}$ represent elements in the weight, observed, and expected matrices, respectively. (Obviously, when diagonal cells contain weights of 0 and off-diagonal cells weights of 1, this equals $\\kappa$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohens_weighted_kappa(anno.annotations[:,0], anno.annotations[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or averaged over the total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_metric_average(cohens_weighted_kappa,anno.annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if the annontation data can be understood as indicating real values, we can assess not agreement, but rather the correlation of values (Pearson's $\\rho$) or correlation of ranks (Spearman's $\\rho$) for pairs of coders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = pairwise_matrix(pearsons_rho, anno.annotations)\n",
    "m = pairwise_matrix(spearmans_rho, anno.annotations)\n",
    "an = seaborn.heatmap(n)\n",
    "plt.show()\n",
    "am = seaborn.heatmap(m)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or averaged over all comparable pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pairwise_metric_average(pearsons_rho,anno.annotations), pairwise_metric_average(spearmans_rho,anno.annotations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Your turn*</span>\n",
    "\n",
    "<span style=\"color:red\">Perform a content annotation survey of some kind in which at least 3 people evaluate and code each piece of content, using Amazon Mechanical Turk as described in the MTurk slides in the Assignment link on Canvas, or by hand with friends.  With the resulting data, calculate, visualize and discuss inter-coder agreement or covariation with appropriate metrics. What does this means for the reliability of human assessments regarding content in your domain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, what if some coders are better than others. The prior measures all rely on the assumption that all coders are equally good. What if some are worse than others? Now we use Rzhetsky et al (2009) and Dawid & Skene's models to make inference about true label classes by downweighting bad or deviant coders. Pyanno provides three relevant models: ModelA, ModelB, and ModelBt. Model A can only be currently run on a balanced 8-coder design, but assesses accuracy purely based on agreement. Model B with $\\theta$s models the relationship between each coder and code. Model B is the Dawid & Skene model from the reading. The following image schematically suggests the relationship between the models. <img src=\"../data/models.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models should provide similar results. To estimate the parameters for any models, we first need to create a new model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new instance of model A, with 4 label classes\n",
    "model = ModelB.create_initial_state(4, 8)\n",
    "# other model parameters are initialized from the model prior\n",
    "print(model.theta)\n",
    "print(model.log_likelihood(anno.annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = model.sample_posterior_over_accuracy(anno.annotations, 200, burn_in_samples=100, thin_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pyanno allows one to use either MLE (maximum likelihood estimation) or MAP (maximum a posteriori estimation) to estimate model parameters. Note that the parameters here correspond to our estimation of the accuracy of each annotator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.map(anno.annotations)\n",
    "print(model.theta)\n",
    "print(model.log_likelihood(anno.annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelB.create_initial_state(4, 8)\n",
    "model.map(anno.annotations)\n",
    "print(model.theta)\n",
    "print(model.log_likelihood(anno.annotations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have model parameters estimated, we can now make inferences about the true label classes. We can calculate the posterior distribution over the true label classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = model.infer_labels(anno.annotations)\n",
    "print(posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn the posterior of the first 100 samples into a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes = []\n",
    "for r in anno.annotations:\n",
    "    v = [0] * len(anno.labels)\n",
    "    votes.append(v)\n",
    "    for a in r:\n",
    "        if a > -1:\n",
    "            v[a] += 1\n",
    "votes_array = np.array(votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize = (15, 10), sharey=True)\n",
    "num_questions = 20\n",
    "\n",
    "seaborn.heatmap(votes_array[:num_questions], annot = True, ax=ax2)\n",
    "seaborn.heatmap(posterior[:num_questions], annot=True, ax =ax1)\n",
    "ax1.set_title(\"Model\")\n",
    "ax2.set_title(\"Votes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This differs markedly from taking annotator scores at face value (Add comparison of average values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = model.sample_posterior_over_accuracy(anno.annotations, 200, burn_in_samples=100, thin_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(samples[0].mean(axis=0))\n",
    "print(samples[0].std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try everything again with ModelBt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new instance of model B, with 4 label classes and 8 annotators.\n",
    "model = ModelBt.create_initial_state(4, 8)\n",
    "print(model.theta)\n",
    "print(model.log_likelihood(anno.annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.map(anno.annotations)\n",
    "print(model.theta)\n",
    "print(model.log_likelihood(anno.annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = model.infer_labels(anno.annotations)\n",
    "print(posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the posterior of the first 10 samples according to ModelBt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = seaborn.heatmap(posterior[:10,])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The property of these scores is that they enable us to identify the most likely code assuming coders of unequal quality, which also allows us to break ties when we know coder identity. For some analyses, we may simply use the posterior themselves rather than the most probably code outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pyanno also allows one to generate artificial data from a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelBt.create_initial_state(4, 3, theta=[0.99,0.75,0.25])\n",
    "#randome generate annotations with 4 label classes and 3 annotators. The accuracy of the three annotators are 0.99, 0.75, and 0.25 respectively.\n",
    "model.generate_annotations(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing coder accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pyanno provides a [graphical user interface](http://docs.enthought.com/uchicago-pyanno/user_guide.html) for making plots. However, it is not compatible with ipython notebooks. Nevertheless, nothing prevents us from making plots using matplotlib. Let's make a plot of the accuracy of each annotator inferred from ModelA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelBt.create_initial_state(4, 8)\n",
    "model.mle(anno.annotations)\n",
    "samples = model.sample_posterior_over_accuracy(anno.annotations, 200, burn_in_samples=100, thin_samples=3)\n",
    "y =  samples.mean(axis=0)#.mean(axis = 1).mean(axis = 1)\n",
    "y_ci = samples.std(axis=0)#.mean(axis = 1).mean(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.errorbar(range(8),y, yerr = y_ci)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example with articles that use the General Social Survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I performed a recent study in which the variables from thousands of articles were associated with those used in the General Social Survey, a widely used population sample, in order to interrogate how social science analyses are performed. Each article was reread and coded by a balanced set of three student coders using a 6 choose 3 design, such that all possible 3-coder-subsets (20) coded an equal number of articles. Coding was performed through a website that allowed students access to the digital article. To evaluate the validity of the student codes, we also recruited a sample of authors associated with 97 of our published articles to fill out the same online survey. \n",
    "\n",
    "Because not all coders coded items with equal accuracy, and because “don’t know” was an optional answer, leading to potential ties, we used a generative, probabilistic model to estimate the maximum a posteriori probability (MAP) prediction that an item’s code is true, which integrates over the estimated accuracy of coders, assuming only that the entire population of coders is slightly more often right than wrong. The model (“Model B”) is based on a simple underlying generation process that directly accounts for the probability that coded values are correct (Rzhetsky et al. 2009). For each coded value j, a set of parameters, denoted γj, represents the probability that each coded value is correct. For the ith coder (i = 1, 2, …, 6), we introduce a matrix of probabilities, denoted λ(i)x|y, that defines the probability that she assigns code x (e.g., Dependent variable) to a GSS variable with correct annotation y. For a perfect coder, the matrix λ(i)x|y would equal the identity matrix and her vote would count most toward the total. For a coder that always codes incorrectly—a “troll”—her matrix λ(i)x|y will have all its value off the diagonal and will only minimally influence the posterior. We co-authored the open source pyanno software that implements this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the data for each content analysis survey regarding how GSS variables were used in a large population of social science articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#anno_vdep = AnnotationsContainer.from_file(missing_values=[-1], filename=\"GSSvariable_testSdependent.csv\")\n",
    "dev = np.loadtxt(fname=\"../data/dataforgssstudy/n7GSSvariable_testSdependent.csv\", dtype=int, delimiter=\",\")\n",
    "anno_dv = AnnotationsContainer.from_array(dev)\n",
    "\n",
    "ind = np.loadtxt(fname=\"../data/dataforgssstudy/n7GSSvariable_testSindependent.csv\", dtype=int, delimiter=\",\")\n",
    "anno_iv = AnnotationsContainer.from_array(ind)\n",
    "\n",
    "cent = np.loadtxt(fname=\"../data/dataforgssstudy/n7GSSvariable_testScentral.csv\", dtype=int, delimiter=\",\")\n",
    "anno_cv = AnnotationsContainer.from_array(cent)\n",
    "\n",
    "cont = np.loadtxt(fname=\"../data/dataforgssstudy/n7GSSvariable_testScontrol.csv\", dtype=int, delimiter=\",\")\n",
    "anno_ctv = AnnotationsContainer.from_array(cont)\n",
    "\n",
    "test = np.loadtxt(fname=\"../data/dataforgssstudy/testH.csv\", dtype=int, delimiter=\",\")\n",
    "anno_test = AnnotationsContainer.from_array(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_dv.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_dv.missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_dv.annotations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's use Cohen's $\\kappa$ to measure agreement between coders..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = pairwise_matrix(cohens_kappa, anno_dv.annotations)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize that..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = seaborn.heatmap(m)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_metric_average(cohens_kappa, anno_dv.annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the statistics on each of the datasets and with Pearson's $\\rho$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [anno_dv.annotations, anno_iv.annotations, anno_cv.annotations, anno_ctv.annotations]\n",
    "ck = [pairwise_matrix(cohens_kappa, anno) for anno in datasets]\n",
    "pr = [pairwise_matrix(pearsons_rho, anno) for anno in datasets]\n",
    "titles = ['DV', 'IV', 'Central Variable', \"Control Variable\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,4)\n",
    "fig.set_size_inches(18, 7)\n",
    "for k, ax, title in zip(ck,axs[0], titles):\n",
    "    seaborn.heatmap(k, ax = ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "for r, ax in zip(pr,axs[1]):\n",
    "    seaborn.heatmap(r, ax = ax)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will compare the student coders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nondiag = (np.eye(6)-np.ones(6))*-1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdevck = pairwise_matrix(cohens_kappa, anno_dv.annotations)\n",
    "xdevpr = pairwise_matrix(pearsons_rho, anno_dv.annotations)\n",
    "\n",
    "xindck = pairwise_matrix(cohens_kappa, anno_iv.annotations)\n",
    "xindpr = pairwise_matrix(pearsons_rho, anno_iv.annotations)\n",
    "\n",
    "xcenck = pairwise_matrix(cohens_kappa, anno_cv.annotations)\n",
    "xcenpr = pairwise_matrix(pearsons_rho, anno_cv.annotations)\n",
    "\n",
    "xconck = pairwise_matrix(cohens_kappa, anno_ctv.annotations)\n",
    "xconpr = pairwise_matrix(pearsons_rho, anno_ctv.annotations)\n",
    "\n",
    "print(np.average(xdevck, weights=nondiag))\n",
    "print(np.average(xdevpr, weights=nondiag))\n",
    "print(np.average(xindck, weights=nondiag))\n",
    "print(np.average(xindpr, weights=nondiag))\n",
    "print(np.average(xcenck, weights=nondiag))\n",
    "print(np.average(xcenpr, weights=nondiag))\n",
    "print(np.average(xconck, weights=nondiag))\n",
    "print(np.average(xconpr, weights=nondiag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to bring in \"gold standard\" data. In this case, this is where we asked authors of the articles to code their own article's variables and compare with our student coders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedata = np.loadtxt(fname=\"../data/dataforgssstudy/gss_mergedataC.txt\", dtype=int, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_merge_dep = AnnotationsContainer.from_array(mergedata[:,0:2])\n",
    "anno_merge_ind = AnnotationsContainer.from_array(mergedata[:,2:4])\n",
    "anno_merge_cen = AnnotationsContainer.from_array(mergedata[:,4:6])\n",
    "anno_merge_con = AnnotationsContainer.from_array(mergedata[:,6:8])\n",
    "anno_merge_dkn = AnnotationsContainer.from_array(mergedata[:,8:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"Dependent variable -- kappa & rho\"\"\")\n",
    "print(cohens_kappa(anno_merge_dep.annotations[:,0], anno_merge_dep.annotations[:,1]))\n",
    "print(pearsons_rho(anno_merge_dep.annotations[:,0], anno_merge_dep.annotations[:,1]))\n",
    "\n",
    "print(\"\\nIndependent variable\")\n",
    "print(cohens_kappa(anno_merge_ind.annotations[:,0], anno_merge_ind.annotations[:,1]))\n",
    "print(pearsons_rho(anno_merge_ind.annotations[:,0], anno_merge_ind.annotations[:,1]))\n",
    "\n",
    "print(\"\\nCentral variable\")\n",
    "print(cohens_kappa(anno_merge_cen.annotations[:,0], anno_merge_cen.annotations[:,1]))\n",
    "print(pearsons_rho(anno_merge_cen.annotations[:,0], anno_merge_cen.annotations[:,1]))\n",
    "\n",
    "print(\"\\nControl variable\")\n",
    "print(cohens_kappa(anno_merge_con.annotations[:,0], anno_merge_con.annotations[:,1]))\n",
    "print(pearsons_rho(anno_merge_con.annotations[:,0], anno_merge_con.annotations[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoah! Student coders and authors viewed articles that were \"central\" or critical to the published argument as fundamentally different (exhibiting negative agreement and correlation). Why? Likely because that researchers recalled what they had _intended_ as their central variables before analysis, but those that _worked out_ became central in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the assessment of the relative values of authors, then student coders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dependent\")\n",
    "print(np.average(anno_merge_dep.annotations[:,0]))\n",
    "print(np.average(anno_merge_dep.annotations[:,1]))\n",
    "\n",
    "print(\"\\nIndependent\")\n",
    "print(np.average(anno_merge_ind.annotations[:,0]))\n",
    "print(np.average(anno_merge_ind.annotations[:,1]))\n",
    "\n",
    "print(\"\\nCentral\")\n",
    "print(np.average(anno_merge_cen.annotations[:,0]))\n",
    "print(np.average(anno_merge_cen.annotations[:,1]))\n",
    "\n",
    "print(\"\\nControl\")\n",
    "print(np.average(anno_merge_con.annotations[:,0]))\n",
    "print(np.average(anno_merge_con.annotations[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we are going to use models to predict the correct annotations\n",
    "\n",
    "Recall that Model A is built for 8 coders, but we have 6. We're going to *hack* it by adding two blank columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negs2 = np.ones((21461, 2), dtype=np.int)*(-1)\n",
    "devA = np.concatenate((dev, negs2), axis=1)\n",
    "devA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_dvA = AnnotationsContainer.from_array(devA)\n",
    "model_devA = ModelA.create_initial_state(2)\n",
    "model_devA.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dvB = ModelB.create_initial_state(2, 6)\n",
    "print(model_dvB.pi)\n",
    "print(model_dvB.log_likelihood(anno_dv.annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dvB.map(anno_dv.annotations)\n",
    "print(model_dvB.pi)\n",
    "print(model_dvB.log_likelihood(anno_dv.annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the posterior distribution over true annotations\n",
    "posterior_dvB = model_dvB.infer_labels(anno_dv.annotations)\n",
    "# each row show the probability of each label class for the\n",
    "# corresponding item\n",
    "print(posterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_dvB = model_dvB.sample_posterior_over_accuracy(anno_dv.annotations, 200, burn_in_samples=100, thin_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can then compute a credible interval for the parameters:\n",
    "ci_dv_mean = samples_dvB[0].mean(axis=0)\n",
    "print(\"Mean\")\n",
    "print(ci_dv_mean)\n",
    "\n",
    "ci_dv_stdev = samples_dvB[0].std(axis=0)\n",
    "print(\"\\nSTD\")\n",
    "print(ci_dv_stdev)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Model B estimates for other variable assessments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "model_testB = ModelB.create_initial_state(2, 6)\n",
    "print(model_testB.log_likelihood(anno_test.annotations))\n",
    "model_testB.map(anno_test.annotations)\n",
    "print(model_testB.pi)\n",
    "print(model_testB.log_likelihood(anno_test.annotations))\n",
    "print(anno_test.annotations.shape)\n",
    "posterior_testB = model_testB.infer_labels(anno_test.annotations)\n",
    "print(posterior_testB.shape)\n",
    "samples_testB = model_testB.sample_posterior_over_accuracy(anno_test.annotations, 200, burn_in_samples=100, thin_samples=3)\n",
    "ci_test_mean = samples_testB[0].mean(axis=0)\n",
    "print(ci_test_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indepedent variables\n",
    "model_ivB = ModelB.create_initial_state(2, 6)\n",
    "print(model_ivB.log_likelihood(anno_iv.annotations))\n",
    "model_ivB.map(anno_iv.annotations)\n",
    "print(model_ivB.pi)\n",
    "print(model_ivB.log_likelihood(anno_iv.annotations))\n",
    "print(anno_iv.annotations.shape)\n",
    "posterior_ivB = model_ivB.infer_labels(anno_iv.annotations)\n",
    "print(posterior_ivB.shape)\n",
    "samples_ivB = model_ivB.sample_posterior_over_accuracy(anno_iv.annotations, 200, burn_in_samples=100, thin_samples=3)\n",
    "ci_iv_mean = samples_ivB[0].mean(axis=0)\n",
    "print(ci_iv_mean)\n",
    "\n",
    "#central variables\n",
    "model_cvB = ModelB.create_initial_state(2, 6)\n",
    "print(model_cvB.log_likelihood(anno_cv.annotations))\n",
    "model_cvB.map(anno_cv.annotations)\n",
    "print(model_cvB.pi)\n",
    "print(model_cvB.log_likelihood(anno_cv.annotations))\n",
    "print(anno_cv.annotations.shape)\n",
    "posterior_cvB = model_cvB.infer_labels(anno_cv.annotations)\n",
    "print(posterior_cvB.shape)\n",
    "samples_cvB = model_cvB.sample_posterior_over_accuracy(anno_cv.annotations, 200, burn_in_samples=100, thin_samples=3)\n",
    "ci_cv_mean = samples_cvB[0].mean(axis=0)\n",
    "print(ci_cv_mean)\n",
    "\n",
    "#control variables\n",
    "model_ctvB = ModelB.create_initial_state(2, 6)\n",
    "print(model_ctvB.log_likelihood(anno_ctv.annotations))\n",
    "model_ctvB.map(anno_ctv.annotations)\n",
    "print(model_ctvB.pi)\n",
    "print(model_ctvB.log_likelihood(anno_ctv.annotations))\n",
    "print(anno_ctv.annotations.shape)\n",
    "posterior_ctvB = model_ctvB.infer_labels(anno_ctv.annotations)\n",
    "print(posterior_ctvB.shape)\n",
    "samples_ctvB = model_ctvB.sample_posterior_over_accuracy(anno_iv.annotations, 200, burn_in_samples=100, thin_samples=3)\n",
    "ci_ctv_mean = samples_ctvB[0].mean(axis=0)\n",
    "print(ci_ctv_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will package up the predicted data into a format we can use for other, subsequent analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(posterior_dvB.shape)\n",
    "print(posterior_ivB.shape)\n",
    "print(posterior_cvB.shape)\n",
    "print(posterior_ctvB.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_annotations = np.concatenate((posterior_dvB, posterior_ivB, posterior_cvB, posterior_ctvB), axis=1) # posterior_dvBt, posterior_ivBt, posterior_cvBt, posterior_ctvBt), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_annotations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These annotations allowed us to uncover the degree to which social scientists alter their models to achieve a better fit...undocumented data mining. The answer was that social scientists did mine their data, but that it likely improved their analysis because change in the social world was the result of greater distortion than undocumented data mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another example analysis looks at a different data set of Hotel Reviews by a variety of patrons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hotels = pandas.read_csv('../data/hot_Reviews.csv', index_col=0)\n",
    "df_hotels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here a rank of 0 is a missing value and to simplify things more we will convert from a 1-10 scale to a 1-5 scale, with 0 as missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hotels = df_hotels.apply(lambda x: x // 2) #integer divide by 2 rounds all values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can visualize all the reviews as a heatmap with the missing values greyed out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (20,20))\n",
    "seaborn.heatmap(df_hotels, cmap='rainbow', ax = ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give the dataframe to pyanno we need to convert to np array and change the nans to intergers, lets use -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_mat = np.array(df_hotels.fillna(-1).as_matrix())\n",
    "anno_hot = AnnotationsContainer.from_array(hot_mat, missing_values=[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_hot.annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_hot.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_hot.missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at coder agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyanno.measures.agreement.labels_frequency(anno_hot.annotations, 6)#6 possible catagories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = pyanno.measures.agreement.confusion_matrix(anno_hot.annotations[:,0], anno_hot.annotations[:,1], 6) #6 possible catagories\n",
    "print(c)\n",
    "ac = seaborn.heatmap(c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most agreement is on 2 i.e. an average hotel and there's little agreement as rating go higher, likely due to scarcity in the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scotts_pi(anno_hot.annotations[:,0], anno_hot.annotations[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "krippendorffs_alpha(anno_hot.annotations[::])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohens_kappa(anno_hot.annotations[:,0], anno_hot.annotations[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = pairwise_matrix(cohens_kappa, anno_hot.annotations)\n",
    "fig, ax = plt.subplots(figsize = (15, 15))\n",
    "seaborn.heatmap(m, ax =ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_hot = ModelBt.create_initial_state(5, 49)\n",
    "# model_hot.mle(anno_hot.annotations)\n",
    "#print(model.theta)\n",
    "# print(model_hot.log_likelihood(anno_hot.annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def makeQuestionComparison(model, anno_target, num_questions = 20):\n",
    "#     votes = []\n",
    "#     for r in anno_target.annotations:\n",
    "#         v = [0] * len(anno_target.labels)\n",
    "#         votes.append(v)\n",
    "#         for a in r:\n",
    "#             if a > -1:\n",
    "#                 v[a] += 1\n",
    "#     votes_array = np.array(votes)\n",
    "#     posterior = model.infer_labels(anno_target.annotations)\n",
    "#     fig, (ax1, ax2) = plt.subplots(ncols=2, figsize = (15, 10), sharey=True)\n",
    "\n",
    "#     seaborn.heatmap(votes_array[:num_questions], annot = True, ax=ax2)\n",
    "#     seaborn.heatmap(np.nan_to_num(posterior,0)[:num_questions], annot=True, ax =ax1)\n",
    "#     ax1.set_title(\"Model\")\n",
    "#     ax2.set_title(\"Votes\")\n",
    "#     return fig, (ax1, ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makeQuestionComparison(model_hot, anno_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Your turn*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, use the results of your content annotation survey to predict high and low-quality analysts, then predict MAP estimates for your codes in question. What do these estimates suggest about the distribution of skill among your coders? How different are these estimates from a majority vote?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
