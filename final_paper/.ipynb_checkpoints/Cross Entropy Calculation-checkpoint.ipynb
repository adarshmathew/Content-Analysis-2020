{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All these packages need to be installed from pip\n",
    "import gensim\t#For word2vec, etc\n",
    "import requests #For downloading our datasets\n",
    "import lucem_illud_2020\n",
    "\n",
    "import numpy as np #For arrays\n",
    "import pandas as pd #Gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import seaborn #Makes the graphics look nicer\n",
    "import sklearn.metrics.pairwise #For cosine similarity\n",
    "import sklearn.manifold #For T-SNE\n",
    "import sklearn.decomposition #For PCA\n",
    "import spacy\n",
    "import copy\n",
    "import nltk\n",
    "\n",
    "#This 'magic' command makes the plots work better\n",
    "#in the notebook, don't use it outside of a notebook.\n",
    "#Also you can ignore the warning\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "import os #For looking through files\n",
    "import os.path #For managing file paths\n",
    "import pickle\n",
    "import dill\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('donald_full.pkl', 'rb') as fin:\n",
    "#     donald_full = pickle.load(fin)\n",
    "    \n",
    "with open('donald_select.pkl', 'rb') as fin:\n",
    "    donald_select = pickle.load(fin)\n",
    "    \n",
    "with open('2gram_model_dict.pkl', 'rb') as fin:\n",
    "    ngram_dict3 = dill.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ce_calc(tranche_val, token_list):\n",
    "    return ngram_dict3[(tranche_val, 2)].entropy(str(token_list))\n",
    "\n",
    "def word_tokenize_cust(word_list, model=nlp, MAX_LEN=1500000):\n",
    "    \n",
    "    tokenized = []\n",
    "    url_list = []\n",
    "    if type(word_list) == list and len(word_list) == 1:\n",
    "        word_list = word_list[0]\n",
    "\n",
    "    if type(word_list) == list:\n",
    "        word_list = ' '.join([str(elem) for elem in word_list]) \n",
    "    # since we're only tokenizing, I remove RAM intensive operations and increase max text size\n",
    "\n",
    "    model.max_length = MAX_LEN\n",
    "    doc = model(word_list, disable=[\"parser\", \"tagger\", \"ner\"])\n",
    "    \n",
    "    for token in doc:\n",
    "        if not token.is_punct and len(token.text.strip()) > 0:\n",
    "            if token.like_url:\n",
    "                url_list.append(token.text)\n",
    "            else:\n",
    "                tokenized.append(token.text)\n",
    "    \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ca1c896d54496bb735986cabe5defd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2386597.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# donald_full['normalized_toks'] = donald_full['body'].progress_apply(lambda x: [lucem_illud_2020.normalizeTokens(w) for w in word_tokenize_cust(str(x))[:30]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donald_select = donald_full[donald_full['created_tranche'].isin([4,5,6,10,11,12,17,18,19])]\n",
    "\n",
    "with open('donald_select.pkl', 'wb') as fout:\n",
    "    dill.dump(donald_select, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('donald_full.pkl', 'wb') as fout:\n",
    "    dill.dump(donald_full, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count, Pool\n",
    " \n",
    "# cores = cpu_count() #Number of CPU cores on your system\n",
    "# partitions = cores #Define as many partitions as you want\n",
    "\n",
    "num_partitions = 18\n",
    "num_cores = cpu_count()\n",
    "\n",
    "def parallelize_dataframe(df, func):\n",
    "    a1,b1,c1,d1,e1,f1,g1,a2,b2,c2,d2,e2,f2,g2,a3,b3,c3,d3,e3,f3,g3 = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, [a1,b1,c1,d1,e1,f1,g1,a2,b2,c2,d2,e2,f2,g2,a3,b3,c3,d3,e3,f3,g3]))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "  \n",
    "def test_func(data):\n",
    "    data['ce_2gram_par'] = data.progress_apply(lambda x: ce_calc(x['created_tranche'], x['normalized_toks']), axis=1)\n",
    "#     data[\"ce_2gram_par\"] = list(map(ce_calc, donald_full_sub['created_tranche'], donald_full_sub['tok_select']))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donald_select = parallelize_dataframe(donald_select, test_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donald_select.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import  Pool\n",
    "\n",
    "def parallelize_dataframe(df, func, n_cores=4):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donald_full_sub = parallelize(donald_full_sub, ce_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donald_full_sub.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
